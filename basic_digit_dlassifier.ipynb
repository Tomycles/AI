{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier \n",
    "\n",
    "In these notes we are going to build a digit classifier by two different approaches. \n",
    "The first one will be by contructing a neural network from scratch, i.e., creating all our own tools with Pytorch and Pythons classes, methods and functions.\n",
    "In the second approach we will use that fast.ai already created all our previous functions and intermediate steps for the neural network. Therefore, we will go one by one trough them and replace them with fastai-builtin functions which will give us the same results but with less effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Regardless of the way we proceed, in order to implement machine learning over our digit classifier we'll apply the SGD technique by following these rules:\n",
    "\n",
    "1. **Initialize** the weights.\n",
    "1. For each image, use these weights to **predict** the digit.\n",
    "1. Based on these predictions, calculate how good the model is (its **loss**).\n",
    "1. Calculate the **gradient**, which measures for each weight, how changing that weight would change the loss\n",
    "1. **Step** (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and **repeat** the process.\n",
    "1. Iterate until you decide to **stop** the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"661pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 660.87 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-74 656.87,-74 656.87,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135.2\" cy=\"-18\" rx=\"44.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.2\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.25,-18C62.37,-18 71.63,-18 80.89,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.89,-21.5 90.89,-18 80.89,-14.5 80.89,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"244.99\" cy=\"-52\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.99\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.6,-28.85C183.05,-32.78 197.09,-37.21 209.54,-41.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.53,-44.49 219.12,-44.16 210.64,-37.81 208.53,-44.49\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"406.63\" cy=\"-52\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.63\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M273.8,-52C293.82,-52 321.57,-52 346.45,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"346.55,-55.5 356.55,-52 346.55,-48.5 346.55,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"524.23\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.23\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M445.8,-40.77C459.01,-36.89 473.76,-32.55 486.82,-28.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"487.82,-32.06 496.43,-25.88 485.85,-25.35 487.82,-32.06\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M493.68,-18C428.65,-18 272.39,-18 189.67,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.47,-14.5 179.47,-18 189.47,-21.5 189.47,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"315.09\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"622.32\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"622.32\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M554.84,-18C563.24,-18 572.53,-18 581.44,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"581.64,-21.5 591.64,-18 581.64,-14.5 581.64,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fbe4c15eeb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seven steps, are the key to the training of all deep learning models (not only for this SGD method!). \n",
    "That deep learning turns out to rely entirely on these steps is extremely surprising and counterintuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the SGD steps, we need the data. \n",
    "To do so we will begin simple, so instead of considering all digits from 0 to 9 of MNIST database we will concentrate on creating a program which distinguishes between 3s and 7s.\n",
    "\n",
    "To do so we start by downloading the images for those two digits. Fastai helps us downloading the images already split into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('valid'),Path('labels.csv'),Path('train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE) # download images from URLs and unzip them into path\n",
    "Path.BASE_PATH = path # renames path = \"storage/data/...\" as path = (\".\") but it isn't really necessary\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we store them into variables after performing the following algorithm:\n",
    "- For every image in the corresponding path, we open it and then convert it into a tensor in which each component represents a pixel of the original image. The white pixels are stored as the number 0, black is the number 255, and shades of gray are between the two. The entire image has 28 X 28 = 784 pixels.\n",
    "- Then we fit all the tensors into a list. To convert that list of tensors into a tensor of rank-3 we use the stack method.\n",
    "- Finally we normalize all the components to run from 0 to 1 by diving by they greatests number, 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]),\n",
       " torch.Size([6265, 28, 28]),\n",
       " torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_3 = torch.stack([tensor(Image.open(o)) for o in (path/\"train\"/\"3\").ls()])\n",
    "test_3 = test_3.float()/255\n",
    "test_7 = torch.stack([tensor(Image.open(o)) for o in (path/\"train\"/\"7\").ls()])\n",
    "test_7 = test_7.float()/255\n",
    "valid_3 = torch.stack([tensor(Image.open(o)) for o in (path/\"valid\"/\"3\").ls()])\n",
    "valid_3 = valid_3.float()/255\n",
    "valid_7 = torch.stack([tensor(Image.open(o)) for o in (path/\"valid\"/\"7\").ls()])\n",
    "valid_7 = valid_7.float()/255\n",
    "test_3.shape,test_7.shape,valid_3.shape, valid_7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rewrite our data in a more efficient way. We will concatenate all the independent variables (tensor of threes and tensor of sevens) and reshape each of the 28x28-dimensional tensors into a 784-dimensional vector (this is achieved by using the view function). \n",
    "For the dependent variables we'll create a vector which labels each three as 1 and each seven as 0.\n",
    "Finally we organize all our dataset into a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "train_x = torch.cat([test_3, test_7]).view(-1, 28*28)\n",
    "train_y = tensor([1]*len(test_3) + [0]*len(test_7)).unsqueeze(1)\n",
    "train_dataset = list(zip(train_x,train_y))\n",
    "# validation set\n",
    "valid_x = torch.cat([valid_3, valid_7]).view(-1, 28*28)\n",
    "valid_y = tensor([1]*len(valid_3) + [0]*len(valid_7)).unsqueeze(1)\n",
    "valid_dataset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch\n",
    "\n",
    "Here we will build a neural network to classify digits from scratch just using Python and Pytorch and creating all the intermediate steps ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initilize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the SGD method is to **initilize** the parameters randomly using torch.randn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "w = init_params((28*28,1))\n",
    "b = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requires_grad_ function is super important. Since we are using SGD, after all the process of learning we'll end up with a function over the parameters, i.e. $f(w,b)$ and we need its gradient in terms of w and b, $\\left.\\nabla_{w,b} f \\right|_{w=w_{\\text{ini}},b=b_{\\text{ini}}}$, evaluated at the values they were initialized. This requires_grad function is keeping track of all the relevant information to carry on the process. It tells Pytorch that w and b will be independent variables of future functions and that, eventually, we'll need the gradient of them evaluated at the intialized value. Before continuing, lets revisit how this function is used in the simplest scenario and how to obtain the value of the gradient at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "# We begin defining a variable x (which has to be a tensor of floats) and ask for keeping track of the gradient \n",
    "# of any function applied over it. The requires_grad=True property is exactly refering to this characteristic.\n",
    "x=tensor(2.).requires_grad_()\n",
    "print(x)\n",
    "# Now we define two functions to then compose them and see how gradients work also for compositions.\n",
    "def f(x): return x**2\n",
    "def g(x): return 5*x + 10\n",
    "# We apply these functions over x and store it inside y. Now the function y(x) has the value and the property\n",
    "# grad_fn which encodes the information of the gradient over the functions applied over x.\n",
    "y= g(f(x))\n",
    "print(y)\n",
    "# Now we compute the gradient\n",
    "y.backward()\n",
    "# and obtain the gradient evaluated at the x point by the attribute grad over x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these initial values we define a function which just computes the product between w and a vector (carried by the @ symbol) and shift it by b. This defines our particular model and we use it over each of the vectors in test_x to **predict** the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20.7097],\n",
       "        [ -6.5004],\n",
       "        [  2.6714],\n",
       "        ...,\n",
       "        [-13.9627],\n",
       "        [ -2.2773],\n",
       "        [-13.0499]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear1(x): return x@w + b\n",
    "predictions = linear1(train_x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define a function which meassures the distance between predictions and targets. This will be the **loss function** which takes the predictions of the model and their correct values (targets). \n",
    "The first step is to apply a sigmoid function on predictions which schrinks their values between 0 and 1. The where function computes how distant each prediction is from 1 if the correct value is 1, or from 0 if the correct value is 0. Then we compute the average of them with mean. (Is this the l1 norm?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our loss function we need to compute **gradients** so then we can use it to update the parameters. For it we need to specify how many data points we are going to use before updating the parameters. This set of points is called mini-batch and its size batch size. The best way to select it among all the test data is by doing it randomly. To do so one takes the training set, shuffles it randomly and just then split it into mini-batches. This procedure assures that for each epoch of the training process, the data from which we compute the gradients is different. The shuffling and mini-batch collection can be done with fastai's `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=256) # WHY DO WE DO THE SAME FOR THE VALIDATION SET?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradients we put everything in a function. Its arguments are a mini-batch (x), its correct values (y) and the model (in our case the linear1 function). By applying the model over the mini-batch we obtain the predictions and then we compute the loss by using the loss function between the predictions and the correct values. Remember that loss(model(x),y) is a (composite) function of the parameters w and b and we are keeping track of its gradient at the initial values of w and b. With `backward` we are effectively computing this gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(x, y, model):\n",
    "    predictions = model(x)\n",
    "    loss = loss_function(predictions, y)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 6. Step and Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining step is to use this gradient to **step** (or **update**) the parameters which is nothing but the training process. We group all the steps in a single function which does the following: For every mini-batch in train_dataloader it computes the gradient of it using model and then, for each parameter, w and b, it updates them by shifting them by the gradients times a learning rate that we have to fix. Notice that this very step is also a function over the parameters, so, in principle, when using backprop, this step would be also taken into account to compute the gradient. This would be undesired and so to avoid it we apply the shift to p.data instead to the whole p variables. Finally, we use grad.zero_() to reset the gradients of p to zero before changing to the next mini-batch, otherwise backprop would keep adding previous gradients to the result yet we want just the one computed for a single mini-batch at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, learning_rate, parameters):\n",
    "    for x,y in train_dataloader:\n",
    "        calculate_gradient(x, y, model)\n",
    "        for p in parameters:\n",
    "            p.data -= p.grad*learning_rate\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to be able to decide when to **stop** training the machine, we need to compute the accuracy of our program using the validation set. To do so, we define a function which applies over a single mini-batch (of the validation set) and computes how many times we predict the correct result and takes its average. Finally we loop over all the batches with another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(x, y):\n",
    "    predictions = x.sigmoid()\n",
    "    correct = (predictions>0.5) == y\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accuracy = [batch_accuracy(model(x), y) for x,y in valid_dataloader]\n",
    "    return round(torch.stack(accuracy).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this validation functions we create our last function which trains the model for any desired epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, parameters, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model,learning_rate,parameters)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by testing how good our model does without been trained, i.e. we initilize the parameters on some random values and we compute the accuracy on the validation set. This should be a number around 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5749"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = init_params((28*28,1))\n",
    "b = init_params(1)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model for 20 epochs and see how it learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6776 0.8226 0.8973 0.9344 0.9432 0.9495 0.952 0.9544 0.9554 0.9579 0.9588 0.9608 0.9623 0.9643 0.9657 0.9662 0.9677 0.9687 0.9687 0.9701 "
     ]
    }
   ],
   "source": [
    "w = init_params((28*28,1))\n",
    "b = init_params(1)\n",
    "lr=1.\n",
    "epochs=20\n",
    "train_model(linear1,lr,(w,b),epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SGD is such a general foundation, PyTorch provides some useful classes to make it easier to implement. For instance, nn.Linear does the same as our init_params and linear1 functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "w,b = linear_model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With it we can create an Optimizer class which has the update step as a function step(). Then we create an object of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): \n",
    "        self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can rewrite our training functions in a simpler form (we still need our calculate_gradient, loss_function  and validation accuracy functions!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for x,y in train_dataloader:\n",
    "        calculate_gradient(x, y, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can compute the same as before but with less work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.6758 0.8618 0.916 0.9375 0.9546 0.9624 0.9653 0.9658 0.9697 0.9717 0.9731 0.9731 0.9751 0.9746 0.9751 0.9765 0.9775 0.9775 0.978 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we notice that we don't have to reinvent the wheel, we can use full fastai functionalities. \n",
    "For instance, fastai has an `SGD` class which does the same as BasicOptim above (we could have just changed opt = SGD(linear_model.parameters(),lr)). \n",
    "Instead of train_model we can use fastai built-in-function Learner.fit. To do so we need a `DataLoaders` object (which is just a concatenation of our train_dataloader and valid_dataloader) and a `Learner`. The latter is created by passing in all the elements that we've created before: the `DataLoaders`, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636430</td>\n",
       "      <td>0.502765</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.378439</td>\n",
       "      <td>0.268043</td>\n",
       "      <td>0.750245</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.144123</td>\n",
       "      <td>0.167544</td>\n",
       "      <td>0.852306</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066729</td>\n",
       "      <td>0.101134</td>\n",
       "      <td>0.914132</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.073380</td>\n",
       "      <td>0.934740</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026887</td>\n",
       "      <td>0.058592</td>\n",
       "      <td>0.952404</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.049671</td>\n",
       "      <td>0.961236</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019924</td>\n",
       "      <td>0.043967</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018693</td>\n",
       "      <td>0.040062</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017896</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>0.968106</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017315</td>\n",
       "      <td>0.034985</td>\n",
       "      <td>0.971541</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.973503</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.030499</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.029434</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = DataLoaders(train_dataloader,valid_dataloader)\n",
    "\n",
    "learn = Learner(dataloaders, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=loss_function, metrics=batch_accuracy)\n",
    "\n",
    "learn.fit(15, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to here we created just a linear classifier. In order to perform more complex task we need to add some non-linearity to the model. This can be achieved by building a neural network with different (linear) layers connected by non-linear functions, the latter are called activation functions. For instance, in this case we can use the rectifier linear unit function or ReLU. Using Pytorch classes and modules we create the following NN with just one hidden layer in which the input layer has 784 entries, the hidden layer has 30 neurons and the output layer has just 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dataloaders, simple_net, opt_func=SGD,\n",
    "                loss_func=loss_function, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.299044</td>\n",
       "      <td>0.408702</td>\n",
       "      <td>0.505888</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.139861</td>\n",
       "      <td>0.210409</td>\n",
       "      <td>0.827772</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077749</td>\n",
       "      <td>0.105149</td>\n",
       "      <td>0.924436</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.071644</td>\n",
       "      <td>0.948479</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039494</td>\n",
       "      <td>0.056593</td>\n",
       "      <td>0.960746</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033305</td>\n",
       "      <td>0.048280</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029711</td>\n",
       "      <td>0.043075</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025662</td>\n",
       "      <td>0.036913</td>\n",
       "      <td>0.969087</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024350</td>\n",
       "      <td>0.034916</td>\n",
       "      <td>0.971050</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023287</td>\n",
       "      <td>0.033320</td>\n",
       "      <td>0.972522</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.032003</td>\n",
       "      <td>0.973503</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020391</td>\n",
       "      <td>0.029075</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.028321</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>0.976448</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.027028</td>\n",
       "      <td>0.977429</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018603</td>\n",
       "      <td>0.026468</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018252</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.025484</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017632</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.017098</td>\n",
       "      <td>0.024276</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016629</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016415</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.016213</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.016021</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.022526</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.022297</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>0.021878</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>0.021687</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.015044</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>0.021334</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014770</td>\n",
       "      <td>0.021172</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.021017</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014394</td>\n",
       "      <td>0.020729</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the accuracy of our model in terms of the epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there is nothing stopping us to go further and further in layers, this approach would improve the performacer of the model making the learning times shorter and shorter. For instance, the following fastai-built-in netwoork with 18 layers achieves almost 100% of accuracy after just one single epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.087639</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>0.997547</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
